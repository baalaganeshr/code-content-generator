# ============================================
# HuggingFace Model Configuration
# ============================================

# The AI model to use for code generation
# Recommended: deepseek-ai/deepseek-coder-6.7b-instruct-awq (4-bit quantized, ~4GB VRAM)
# Alternative: codellama/CodeLlama-7b-Instruct-hf (requires ~8GB VRAM)
HF_MODEL=deepseek-ai/deepseek-coder-6.7b-instruct-awq

# HuggingFace API token (optional for public models, required for gated models)
# Get your token from: https://huggingface.co/settings/tokens
# Leave empty if not needed
HF_TOKEN=

# ============================================
# Hardware Configuration
# ============================================

# Compute device for model inference
# Options: 
#   - "cuda" = Use NVIDIA GPU (requires CUDA-compatible GPU and drivers)
#   - "cpu"  = Use CPU (slower, but works everywhere)
DEVICE=cuda

# Model quantization level
# Options:
#   - "4bit" = 4-bit quantization (recommended, ~4GB VRAM)
#   - "8bit" = 8-bit quantization (~8GB VRAM)
#   - ""     = No quantization (requires full model size in VRAM)
QUANTIZATION=4bit

# ============================================
# Generation Parameters
# ============================================

# Maximum number of tokens to generate per AI request
# Higher = longer responses but slower generation
# Range: 512-4096 (recommended: 2048)
MAX_LENGTH=2048

# Temperature for text generation
# Controls randomness/creativity of output
# Range: 0.1-1.0
#   - Lower (0.1-0.4) = More focused, deterministic, consistent
#   - Medium (0.5-0.7) = Balanced creativity and consistency
#   - Higher (0.8-1.0) = More creative, varied, random
TEMPERATURE=0.7

# ============================================
# API Configuration (optional)
# ============================================

# Backend API server port
# Default: 8000
API_PORT=8000

# Backend API server host
# Default: 0.0.0.0 (all interfaces)
API_HOST=0.0.0.0

# ============================================
# Frontend Configuration (optional)
# ============================================

# Frontend web server port
# Default: 80 (change to 3000 if port 80 is in use)
FRONTEND_PORT=80

# ============================================
# Advanced Settings (optional)
# ============================================

# Enable debug logging
# Options: "true", "false"
DEBUG=false

# Log level
# Options: "DEBUG", "INFO", "WARNING", "ERROR"
LOG_LEVEL=INFO

